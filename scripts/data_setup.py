#!/usr/bin/env python3
"""
ETH Price Prediction Data Setup Script

This script orchestrates the collection of all required data sources for the ETH prediction pipeline.
It uses the individual data collection scripts to fetch and organize data in the expected folder structure.

Usage:
    python scripts/data_setup.py                    # Setup all data sources
    python scripts/data_setup.py --source binance   # Setup specific source
    python scripts/data_setup.py --validate         # Validate existing data
    python scripts/data_setup.py --force            # Force re-download all data
"""

import os
import sys
import argparse
import logging
import traceback
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd
import time
from typing import Dict, List, Optional, Tuple

# Add scripts directory to path for imports
sys.path.append(str(Path(__file__).parent))

try:
    from get_eth_data import main as get_eth_data_main
    from defillama_data import main as defillama_main
    from santiment_data import main as santiment_main  
    from binance_data import main as binance_main
    DATA_SCRIPTS_AVAILABLE = True
except ImportError as e:
    DATA_SCRIPTS_AVAILABLE = False
    IMPORT_ERROR = str(e)


class DataSetupManager:
    """Manages the complete data setup process for ETH prediction pipeline."""
    
    def __init__(self, data_dir: str = "data", force_download: bool = False):
        self.data_dir = Path(data_dir)
        self.force_download = force_download
        self.setup_logging()
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # Expected data files and their sources
        self.data_sources = {
                         'binance': {
                 'script': 'binance_data.py',
                 'files': [
                     'raw/ETHUSDT-1h-2025-04.csv',
                     'raw/ETHUSDT-1h-2025-05.csv'
                 ],
                 'description': 'Binance ETH/USDT hourly price data'
             },
                         'defillama': {
                 'script': 'defillama_data.py', 
                 'files': [
                     'defillama_eth_chain_tvl_2025_04-05.csv'
                 ],
                 'description': 'DeFiLlama chain TVL data'
             },
            'santiment': {
                'script': 'santiment_data.py',
                'files': [
                    'santiment_metrics_april_may_2025.csv'
                ],
                'description': 'Santiment on-chain metrics'
            }
        }
        
        self.setup_results = {}
        
    def setup_logging(self):
        """Setup logging for data setup process."""
        # Create logs directory
        Path("logs").mkdir(exist_ok=True)
        
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'logs/data_setup_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
    def create_directory_structure(self) -> bool:
        """Create the required directory structure."""
        self.logger.info("=== Creating Directory Structure ===")
        
        try:
            # Main data directory
            self.data_dir.mkdir(exist_ok=True)
            self.logger.info(f"✓ Created main data directory: {self.data_dir}")
            
            # Subdirectories
            subdirs = ['raw', 'processed', 'interim', 'external']
            for subdir in subdirs:
                (self.data_dir / subdir).mkdir(exist_ok=True)
                self.logger.info(f"✓ Created subdirectory: {subdir}")
            
            # Create README file
            readme_content = """# ETH Price Prediction Data Directory

This directory contains all data sources for the ETH price prediction pipeline.

## Structure:
- raw/: Raw data files as downloaded
- processed/: Cleaned and processed data files  
- interim/: Intermediate processing files
- external/: External reference data

## Data Sources:
- Binance: ETH/USDT price and volume data
- DeFiLlama: TVL and protocol metrics
- Santiment: On-chain metrics and social data

Generated by data_setup.py on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
            
            readme_path = self.data_dir / "README.md"
            with open(readme_path, 'w') as f:
                f.write(readme_content)
            
            self.logger.info("✓ Directory structure created successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to create directory structure: {e}")
            return False
    
    def validate_existing_data(self) -> Dict[str, Dict[str, any]]:
        """Validate existing data files."""
        self.logger.info("=== Validating Existing Data ===")
        
        validation_results = {}
        
        for source_name, source_info in self.data_sources.items():
            self.logger.info(f"\nValidating {source_name} data...")
            
            source_results = {
                'files_found': 0,
                'files_missing': [],
                'files_info': {},
                'total_size_mb': 0,
                'newest_file': None,
                'oldest_file': None
            }
            
            file_dates = []
            
            for file_path in source_info['files']:
                full_path = self.data_dir / file_path
                
                if full_path.exists():
                    # Get file info
                    stat = full_path.stat()
                    size_mb = stat.st_size / (1024 * 1024)
                    mod_time = datetime.fromtimestamp(stat.st_mtime)
                    
                    source_results['files_found'] += 1
                    source_results['total_size_mb'] += size_mb
                    source_results['files_info'][file_path] = {
                        'size_mb': size_mb,
                        'modified': mod_time,
                        'exists': True
                    }
                    
                    file_dates.append(mod_time)
                    
                    self.logger.info(f"  ✓ {file_path} ({size_mb:.2f} MB)")
                    
                    # Basic data validation for CSV files
                    if file_path.endswith('.csv'):
                        try:
                            df = pd.read_csv(full_path, nrows=5)  # Just check first few rows
                            rows = len(pd.read_csv(full_path))
                            self.logger.info(f"    → {rows} rows, {len(df.columns)} columns")
                        except Exception as e:
                            self.logger.warning(f"    → Could not validate CSV: {e}")
                            
                else:
                    source_results['files_missing'].append(file_path)
                    source_results['files_info'][file_path] = {'exists': False}
                    self.logger.warning(f"  ✗ Missing: {file_path}")
            
            # Set date range
            if file_dates:
                source_results['newest_file'] = max(file_dates)
                source_results['oldest_file'] = min(file_dates)
            
            validation_results[source_name] = source_results
            
            # Summary for this source
            total_files = len(source_info['files'])
            found_files = source_results['files_found']
            
            if found_files == total_files:
                self.logger.info(f"✓ {source_name}: All {total_files} files found ({source_results['total_size_mb']:.2f} MB total)")
            else:
                self.logger.warning(f"✗ {source_name}: {found_files}/{total_files} files found")
        
        return validation_results
    
    def setup_binance_data(self) -> bool:
        """Setup Binance price data."""
        self.logger.info("=== Setting up Binance Data ===")
        
        if not DATA_SCRIPTS_AVAILABLE:
            raise RuntimeError(f"Data collection scripts not available: {IMPORT_ERROR}")
        
        try:
            # Call the Binance data script
            self.logger.info("Executing Binance data collection script...")
            binance_main()
            self.logger.info("✓ Binance data collection completed")
            
            return self._validate_binance_data()
            
        except Exception as e:
            self.logger.error(f"Binance data setup failed: {e}")
            self.logger.debug(traceback.format_exc())
            raise RuntimeError(f"Failed to setup Binance data: {e}") from e
    
    def setup_defillama_data(self) -> bool:
        """Setup DeFiLlama TVL data."""
        self.logger.info("=== Setting up DeFiLlama Data ===")
        
        if not DATA_SCRIPTS_AVAILABLE:
            raise RuntimeError(f"Data collection scripts not available: {IMPORT_ERROR}")
        
        try:
            # Call the DeFiLlama data script
            self.logger.info("Executing DeFiLlama data collection script...")
            defillama_main()
            self.logger.info("✓ DeFiLlama data collection completed")
            
            return self._validate_defillama_data()
            
        except Exception as e:
            self.logger.error(f"DeFiLlama data setup failed: {e}")
            self.logger.debug(traceback.format_exc())
            raise RuntimeError(f"Failed to setup DeFiLlama data: {e}") from e
    
    def setup_santiment_data(self) -> bool:
        """Setup Santiment on-chain data."""
        self.logger.info("=== Setting up Santiment Data ===")
        
        if not DATA_SCRIPTS_AVAILABLE:
            raise RuntimeError(f"Data collection scripts not available: {IMPORT_ERROR}")
        
        try:
            # Call the Santiment data script
            self.logger.info("Executing Santiment data collection script...")
            santiment_main()
            self.logger.info("✓ Santiment data collection completed")
            
            return self._validate_santiment_data()
            
        except Exception as e:
            self.logger.error(f"Santiment data setup failed: {e}")
            self.logger.debug(traceback.format_exc())
            raise RuntimeError(f"Failed to setup Santiment data: {e}") from e
    

    
    def _validate_binance_data(self) -> bool:
        """Validate Binance data files."""
        expected_files = self.data_sources['binance']['files']
        
        for file_path in expected_files:
            full_path = self.data_dir / file_path
            if not full_path.exists():
                self.logger.error(f"Missing Binance file: {file_path}")
                return False
            
            # Basic validation
            try:
                df = pd.read_csv(full_path)
                required_cols = ['open_time', 'open', 'high', 'low', 'close', 'volume']
                
                if not all(col in df.columns for col in required_cols):
                    self.logger.error(f"Missing required columns in {file_path}")
                    return False
                
                if len(df) < 100:  # Should have reasonable amount of data
                    self.logger.error(f"Insufficient data in {file_path}: {len(df)} rows")
                    return False
                
                self.logger.info(f"✓ Validated {file_path}: {len(df)} rows")
                
            except Exception as e:
                self.logger.error(f"Failed to validate {file_path}: {e}")
                return False
        
        return True
    
    def _validate_defillama_data(self) -> bool:
        """Validate DeFiLlama data files."""
        expected_files = self.data_sources['defillama']['files']
        
        for file_path in expected_files:
            full_path = self.data_dir / file_path
            if not full_path.exists():
                self.logger.error(f"Missing DeFiLlama file: {file_path}")
                return False
            
            try:
                df = pd.read_csv(full_path)
                
                if len(df) < 10:
                    self.logger.error(f"Insufficient data in {file_path}: {len(df)} rows")
                    return False
                
                self.logger.info(f"✓ Validated {file_path}: {len(df)} rows")
                
            except Exception as e:
                self.logger.error(f"Failed to validate {file_path}: {e}")
                return False
        
        return True
    
    def _validate_santiment_data(self) -> bool:
        """Validate Santiment data files."""
        expected_files = self.data_sources['santiment']['files']
        
        for file_path in expected_files:
            full_path = self.data_dir / file_path
            if not full_path.exists():
                self.logger.error(f"Missing Santiment file: {file_path}")
                return False
            
            try:
                df = pd.read_csv(full_path)
                
                # Check for required columns
                required_cols = ['datetime', 'daily_active_addresses_value', 'dev_activity_value']
                if not all(col in df.columns for col in required_cols):
                    self.logger.error(f"Missing required columns in {file_path}")
                    return False
                
                if len(df) < 30:  # Should have at least a month of data
                    self.logger.error(f"Insufficient data in {file_path}: {len(df)} rows")
                    return False
                
                self.logger.info(f"✓ Validated {file_path}: {len(df)} rows")
                
            except Exception as e:
                self.logger.error(f"Failed to validate {file_path}: {e}")
                return False
        
        return True
    
    def setup_all_data(self, sources: Optional[List[str]] = None) -> bool:
        """Setup all data sources."""
        self.logger.info("=" * 60)
        self.logger.info("STARTING ETH PREDICTION DATA SETUP")
        self.logger.info("=" * 60)
        
        # Check script availability first
        if not DATA_SCRIPTS_AVAILABLE:
            self.logger.error("=" * 60)
            self.logger.error("CRITICAL ERROR: Data collection scripts not available!")
            self.logger.error("=" * 60)
            self.logger.error(f"Import error: {IMPORT_ERROR}")
            self.logger.error("")
            self.logger.error("Please ensure all data collection scripts are present in the scripts/ folder:")
            for source, info in self.data_sources.items():
                self.logger.error(f"  - {info['script']}: {info['description']}")
            self.logger.error("")
            self.logger.error("Available scripts should include:")
            self.logger.error("  - binance_data.py (Binance price data)")
            self.logger.error("  - defillama_data.py (DeFiLlama TVL data)")
            self.logger.error("  - santiment_data.py (Santiment on-chain metrics)")
            self.logger.error("  - get_eth_data.py (ETH data aggregator)")
            raise RuntimeError("Data collection scripts not properly set up")
        
        start_time = datetime.now()
        
        # Create directory structure
        if not self.create_directory_structure():
            raise RuntimeError("Failed to create directory structure")
        
        # Determine which sources to setup
        if sources is None:
            sources = list(self.data_sources.keys())
        
        # Setup each data source
        setup_functions = {
            'binance': self.setup_binance_data,
            'defillama': self.setup_defillama_data,
            'santiment': self.setup_santiment_data
        }
        
        success_count = 0
        total_sources = len(sources)
        
        for source in sources:
            if source in setup_functions:
                self.logger.info(f"\n{'='*20} Setting up {source.title()} {'='*20}")
                
                try:
                    if setup_functions[source]():
                        success_count += 1
                        self.setup_results[source] = 'SUCCESS'
                        self.logger.info(f"✓ {source.title()} setup completed successfully")
                    else:
                        self.setup_results[source] = 'FAILED'
                        self.logger.error(f"✗ {source.title()} setup failed")
                        
                except RuntimeError as e:
                    self.setup_results[source] = f'CRITICAL ERROR: {str(e)}'
                    self.logger.error(f"✗ {source.title()} setup failed critically: {e}")
                    # Re-raise critical errors to stop execution
                    raise
                except Exception as e:
                    self.setup_results[source] = f'ERROR: {str(e)}'
                    self.logger.error(f"✗ {source.title()} setup crashed: {e}")
                    raise RuntimeError(f"Unexpected error in {source} setup: {e}") from e
                
                # Small delay between sources
                time.sleep(1)
            else:
                self.logger.warning(f"Unknown data source: {source}")
        
        # Final validation
        self.logger.info(f"\n{'='*20} Final Validation {'='*20}")
        validation_results = self.validate_existing_data()
        
        # Summary
        duration = datetime.now() - start_time
        
        self.logger.info("\n" + "=" * 60)
        self.logger.info("DATA SETUP SUMMARY")
        self.logger.info("=" * 60)
        self.logger.info(f"Duration: {duration}")
        self.logger.info(f"Sources processed: {total_sources}")
        self.logger.info(f"Successful setups: {success_count}")
        
        for source, result in self.setup_results.items():
            self.logger.info(f"  {source}: {result}")
        
        # Data file summary
        total_files = 0
        found_files = 0
        total_size = 0
        
        for source, validation in validation_results.items():
            total_files += len(self.data_sources[source]['files'])
            found_files += validation['files_found']
            total_size += validation['total_size_mb']
        
        self.logger.info(f"\nData files: {found_files}/{total_files} found ({total_size:.2f} MB total)")
        
        success = success_count == total_sources and found_files == total_files
        
        if success:
            self.logger.info("\n🎉 DATA SETUP COMPLETED SUCCESSFULLY!")
            self.logger.info("All data sources are ready for the ETH prediction pipeline.")
        else:
            self.logger.error(f"\n❌ DATA SETUP INCOMPLETE!")
            self.logger.error("Some data sources failed. Check logs for details.")
        
        return success


def main():
    """Main entry point for data setup."""
    parser = argparse.ArgumentParser(
        description="ETH Price Prediction Data Setup Script",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python scripts/data_setup.py                     # Setup all data sources
  python scripts/data_setup.py --source binance    # Setup only Binance data
  python scripts/data_setup.py --validate          # Validate existing data only
  python scripts/data_setup.py --force             # Force re-download all data
        """
    )
    
    parser.add_argument("--source", choices=['binance', 'defillama', 'santiment'],
                       help="Setup specific data source only")
    parser.add_argument("--validate", action="store_true",
                       help="Validate existing data without downloading")
    parser.add_argument("--force", action="store_true",
                       help="Force re-download all data")
    parser.add_argument("--data-dir", default="data",
                       help="Data directory path (default: data)")
    
    args = parser.parse_args()
    
    # Initialize data setup manager
    manager = DataSetupManager(data_dir=args.data_dir, force_download=args.force)
    
    if args.validate:
        # Validation only
        manager.logger.info("Running data validation only...")
        validation_results = manager.validate_existing_data()
        
        # Check if all data is present
        all_present = True
        for source_name, source_info in manager.data_sources.items():
            validation = validation_results[source_name]
            if validation['files_found'] != len(source_info['files']):
                all_present = False
                break
        
        if all_present:
            manager.logger.info("✓ All data files are present and valid")
            sys.exit(0)
        else:
            manager.logger.error("✗ Some data files are missing or invalid")
            sys.exit(1)
    
    else:
        # Setup data
        sources = [args.source] if args.source else None
        success = manager.setup_all_data(sources=sources)
        sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
